{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HQF-DE: Document Expansion Pipeline\n",
    "\n",
    "This notebook expands MS MARCO documents using the HQF-DE pipeline:\n",
    "\n",
    "1. **LLM** (Llama-3-8B) → Identifies semantic gaps and generates expansions\n",
    "2. **NLI** (DeBERTa) → Validates expansions are factually consistent\n",
    "3. **Doc2Query** (T5) → Generates synthetic queries\n",
    "4. **Combiner** → Deduplicates and selects best expansions\n",
    "\n",
    "**Input:** `collection_subset.tsv` (1M docs, 331MB)  \n",
    "**Output:** `expanded_hqfde.tsv`, `expanded_d2q.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 20 22:30:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   35C    P0             49W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers sentence-transformers accelerate bitsandbytes pydantic-settings sentencepiece protobuf tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace (required for Llama-3)\n# Run this cell, paste your token in the text box, and press Enter\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nimport os\nfrom huggingface_hub import login\n\ndef input_secret(secret_name):\n    def handle_submit(sender):\n        os.environ[secret_name] = sender.value\n        login(token=sender.value)\n        with output_area:\n            clear_output()\n            print(f\"Logged in to HuggingFace!\")\n\n    text_input = widgets.Text(\n        value='',\n        placeholder='Paste your HuggingFace token here',\n        description='HF Token:',\n        disabled=False,\n        style={'description_width': 'initial'}\n    )\n    output_area = widgets.Output()\n    text_input.on_submit(handle_submit)\n    \n    with output_area:\n        display(text_input)\n    display(output_area)\n\ninput_secret(\"HF_TOKEN\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup data paths\nimport os\nos.makedirs('/content/data', exist_ok=True)\nos.makedirs('/content/output', exist_ok=True)\n\nINPUT_FILE = \"/content/data/collection_subset.tsv\"\nOUTPUT_HQFDE = \"/content/output/expanded_hqfde.tsv\"\nOUTPUT_D2Q = \"/content/output/expanded_d2q.tsv\"\n\n# Option 1: If file already uploaded to runtime\nif os.path.exists(INPUT_FILE):\n    print(f\"Found: {INPUT_FILE}\")\n    !wc -l {INPUT_FILE}\nelse:\n    # Option 2: Download from Google Drive using shareable link\n    # 1. Right-click collection_subset.tsv in Drive → Share → Copy link\n    # 2. Replace FILE_ID below with the ID from the link\n    #    Link format: https://drive.google.com/file/d/FILE_ID/view\n    \n    FILE_ID = \"YOUR_FILE_ID_HERE\"  # Replace with your file ID\n    \n    if FILE_ID != \"YOUR_FILE_ID_HERE\":\n        !pip install -q gdown\n        import gdown\n        gdown.download(f\"https://drive.google.com/uc?id={FILE_ID}\", INPUT_FILE, quiet=False)\n    else:\n        print(\"File not found. To download from Drive:\")\n        print(\"1. Share collection_subset.tsv (Anyone with link)\")\n        print(\"2. Copy the FILE_ID from the share link\")\n        print(\"3. Replace YOUR_FILE_ID_HERE above and re-run\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "CACHE = \"/content/cache\"\n",
    "\n",
    "# Load Llama-3-8B (4-bit quantized)\n",
    "print(\"Loading Llama-3-8B...\")\n",
    "llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_name, cache_dir=CACHE)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name, cache_dir=CACHE,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_new_tokens=256)\n",
    "\n",
    "# Load NLI (DeBERTa)\n",
    "print(\"Loading DeBERTa NLI...\")\n",
    "nli_pipe = pipeline(\"text-classification\", model=\"microsoft/deberta-v3-large-mnli\", device=0, top_k=None)\n",
    "\n",
    "# Load Doc2Query (T5)\n",
    "print(\"Loading Doc2Query...\")\n",
    "d2q_tokenizer = T5Tokenizer.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE)\n",
    "d2q_model = T5ForConditionalGeneration.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE).to(DEVICE)\n",
    "\n",
    "# Load Sentence-BERT\n",
    "print(\"Loading Sentence-BERT...\")\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "\n",
    "print(\"All models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Expansion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_expand(doc):\n",
    "    \"\"\"Use LLM to identify gaps and generate expansions.\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Analyze this document and generate 3-5 brief factual expansions that add missing context:\n",
    "{doc}\n",
    "\n",
    "Expansions:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    result = llm_pipe(prompt, return_full_text=False, pad_token_id=llm_tokenizer.pad_token_id)\n",
    "    text = result[0][\"generated_text\"]\n",
    "    \n",
    "    # Parse numbered/bulleted list\n",
    "    expansions = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 10:\n",
    "            if line[0].isdigit():\n",
    "                line = line.split(\".\", 1)[-1].strip()\n",
    "            if line.startswith(\"-\"):\n",
    "                line = line[1:].strip()\n",
    "            if line:\n",
    "                expansions.append(line)\n",
    "    return expansions[:5]\n",
    "\n",
    "\n",
    "def nli_validate(doc, expansions, threshold=0.7):\n",
    "    \"\"\"Filter expansions using NLI entailment.\"\"\"\n",
    "    valid = []\n",
    "    for exp in expansions:\n",
    "        try:\n",
    "            result = nli_pipe(f\"{doc} [SEP] {exp}\", truncation=True, max_length=512)\n",
    "            scores = {r[\"label\"].lower(): r[\"score\"] for r in result}\n",
    "            if scores.get(\"entailment\", 0) >= threshold:\n",
    "                valid.append(exp)\n",
    "        except:\n",
    "            pass\n",
    "    return valid\n",
    "\n",
    "\n",
    "def doc2query(doc, n=5):\n",
    "    \"\"\"Generate synthetic queries.\"\"\"\n",
    "    inputs = d2q_tokenizer(doc, max_length=512, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = d2q_model.generate(**inputs, max_length=64, do_sample=True, top_k=10, num_return_sequences=n)\n",
    "    queries = []\n",
    "    for out in outputs:\n",
    "        q = d2q_tokenizer.decode(out, skip_special_tokens=True).strip()\n",
    "        if q and q not in queries:\n",
    "            queries.append(q)\n",
    "    return queries\n",
    "\n",
    "\n",
    "def deduplicate(doc, expansions, threshold=0.85):\n",
    "    \"\"\"Remove duplicate and redundant expansions.\"\"\"\n",
    "    if not expansions:\n",
    "        return []\n",
    "    \n",
    "    embs = sbert.encode(expansions, normalize_embeddings=True)\n",
    "    sim = cosine_similarity(embs)\n",
    "    kept = []\n",
    "    removed = set()\n",
    "    for i in range(len(expansions)):\n",
    "        if i not in removed:\n",
    "            kept.append(expansions[i])\n",
    "            for j in range(i+1, len(expansions)):\n",
    "                if sim[i,j] >= threshold:\n",
    "                    removed.add(j)\n",
    "    \n",
    "    if kept:\n",
    "        doc_emb = sbert.encode([doc], normalize_embeddings=True)\n",
    "        exp_embs = sbert.encode(kept, normalize_embeddings=True)\n",
    "        sims = cosine_similarity(exp_embs, doc_emb).flatten()\n",
    "        kept = [e for e, s in zip(kept, sims) if s < threshold]\n",
    "    \n",
    "    return kept[:10]\n",
    "\n",
    "\n",
    "def expand_hqfde(doc_id, doc):\n",
    "    \"\"\"Full HQF-DE pipeline.\"\"\"\n",
    "    expansions = llm_expand(doc)\n",
    "    valid = nli_validate(doc, expansions)\n",
    "    queries = doc2query(doc)\n",
    "    final = deduplicate(doc, valid + queries)\n",
    "    return doc_id, f\"{doc} {' '.join(final)}\"\n",
    "\n",
    "\n",
    "def expand_d2q_only(doc_id, doc):\n",
    "    \"\"\"Doc2Query baseline only.\"\"\"\n",
    "    queries = doc2query(doc)\n",
    "    return doc_id, f\"{doc} {' '.join(queries)}\"\n",
    "\n",
    "\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test on Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\"\n",
    "\n",
    "print(\"Original:\", test_doc)\n",
    "print()\n",
    "\n",
    "expansions = llm_expand(test_doc)\n",
    "print(\"LLM Expansions:\")\n",
    "for e in expansions:\n",
    "    print(f\"  - {e}\")\n",
    "\n",
    "valid = nli_validate(test_doc, expansions)\n",
    "print(f\"\\nNLI Valid ({len(valid)}/{len(expansions)}):\")\n",
    "for e in valid:\n",
    "    print(f\"  + {e}\")\n",
    "\n",
    "queries = doc2query(test_doc)\n",
    "print(\"\\nDoc2Query:\")\n",
    "for q in queries:\n",
    "    print(f\"  ? {q}\")\n",
    "\n",
    "_, expanded = expand_hqfde(\"test\", test_doc)\n",
    "print(\"\\nFinal Expanded:\")\n",
    "print(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "LIMIT = 100000  # Process 100K docs (set to None for all 1M)\n",
    "\n",
    "# Load documents\n",
    "docs = []\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    for row in csv.reader(f, delimiter='\\t'):\n",
    "        if len(row) >= 2:\n",
    "            docs.append((row[0], row[1]))\n",
    "            if LIMIT and len(docs) >= LIMIT:\n",
    "                break\n",
    "\n",
    "print(f\"Loaded {len(docs):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HQF-DE expansion\n",
    "print(f\"Running HQF-DE expansion on {len(docs):,} docs...\")\n",
    "print(f\"Output: {OUTPUT_HQFDE}\")\n",
    "\n",
    "with open(OUTPUT_HQFDE, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for doc_id, text in tqdm(docs):\n",
    "        try:\n",
    "            did, expanded = expand_hqfde(doc_id, text)\n",
    "            writer.writerow([did, expanded])\n",
    "        except Exception as e:\n",
    "            writer.writerow([doc_id, text])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Doc2Query baseline (for comparison)\n",
    "print(f\"Running Doc2Query baseline on {len(docs):,} docs...\")\n",
    "print(f\"Output: {OUTPUT_D2Q}\")\n",
    "\n",
    "with open(OUTPUT_D2Q, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for doc_id, text in tqdm(docs):\n",
    "        try:\n",
    "            did, expanded = expand_d2q_only(doc_id, text)\n",
    "            writer.writerow([did, expanded])\n",
    "        except Exception as e:\n",
    "            writer.writerow([doc_id, text])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Output files saved to Google Drive:\")\n",
    "print(f\"  HQF-DE: {OUTPUT_HQFDE} ({os.path.getsize(OUTPUT_HQFDE)/1e6:.1f} MB)\")\n",
    "print(f\"  D2Q:    {OUTPUT_D2Q} ({os.path.getsize(OUTPUT_D2Q)/1e6:.1f} MB)\")\n",
    "print()\n",
    "print(\"Files are automatically synced to your Google Drive.\")\n",
    "print(\"Next: Use these with your C++ indexer for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}