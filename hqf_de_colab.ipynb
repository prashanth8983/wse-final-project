{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HQF-DE: Document Expansion Pipeline\n",
    "\n",
    "This notebook expands MS MARCO documents using the HQF-DE pipeline:\n",
    "\n",
    "1. **LLM** (Llama-3-8B) → Identifies semantic gaps and generates expansions\n",
    "2. **NLI** (DeBERTa) → Validates expansions are factually consistent\n",
    "3. **Doc2Query** (T5) → Generates synthetic queries\n",
    "4. **Combiner** → Deduplicates and selects best expansions\n",
    "\n",
    "**Input:** `collection_subset.tsv` (1M docs, 331MB)  \n",
    "**Output:** `expanded_hqfde.tsv`, `expanded_d2q.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers sentence-transformers accelerate bitsandbytes pydantic-settings sentencepiece protobuf tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (required for Llama-3)\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your token when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload collection_subset.tsv\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "os.makedirs('/content/output', exist_ok=True)\n",
    "\n",
    "print(\"Upload collection_subset.tsv (1M documents, ~331MB)\")\n",
    "uploaded = files.upload()\n",
    "for f in uploaded:\n",
    "    os.rename(f, '/content/data/collection_subset.tsv')\n",
    "    print(f\"Saved to /content/data/collection_subset.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "CACHE = \"/content/cache\"\n",
    "\n",
    "# Load Llama-3-8B (4-bit quantized)\n",
    "print(\"Loading Llama-3-8B...\")\n",
    "llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_name, cache_dir=CACHE)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name, cache_dir=CACHE,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_new_tokens=256)\n",
    "\n",
    "# Load NLI (DeBERTa)\n",
    "print(\"Loading DeBERTa NLI...\")\n",
    "nli_pipe = pipeline(\"text-classification\", model=\"microsoft/deberta-v3-large-mnli\", device=0, top_k=None)\n",
    "\n",
    "# Load Doc2Query (T5)\n",
    "print(\"Loading Doc2Query...\")\n",
    "d2q_tokenizer = T5Tokenizer.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE)\n",
    "d2q_model = T5ForConditionalGeneration.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE).to(DEVICE)\n",
    "\n",
    "# Load Sentence-BERT\n",
    "print(\"Loading Sentence-BERT...\")\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "\n",
    "print(\"All models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Expansion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_expand(doc):\n",
    "    \"\"\"Use LLM to identify gaps and generate expansions.\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Analyze this document and generate 3-5 brief factual expansions that add missing context:\n",
    "{doc}\n",
    "\n",
    "Expansions:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    result = llm_pipe(prompt, return_full_text=False, pad_token_id=llm_tokenizer.pad_token_id)\n",
    "    text = result[0][\"generated_text\"]\n",
    "    \n",
    "    # Parse numbered/bulleted list\n",
    "    expansions = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 10:\n",
    "            # Remove numbering\n",
    "            if line[0].isdigit():\n",
    "                line = line.split(\".\", 1)[-1].strip()\n",
    "            if line.startswith(\"-\"):\n",
    "                line = line[1:].strip()\n",
    "            if line:\n",
    "                expansions.append(line)\n",
    "    return expansions[:5]\n",
    "\n",
    "\n",
    "def nli_validate(doc, expansions, threshold=0.7):\n",
    "    \"\"\"Filter expansions using NLI entailment.\"\"\"\n",
    "    valid = []\n",
    "    for exp in expansions:\n",
    "        try:\n",
    "            result = nli_pipe(f\"{doc} [SEP] {exp}\", truncation=True, max_length=512)\n",
    "            scores = {r[\"label\"].lower(): r[\"score\"] for r in result}\n",
    "            if scores.get(\"entailment\", 0) >= threshold:\n",
    "                valid.append(exp)\n",
    "        except:\n",
    "            pass\n",
    "    return valid\n",
    "\n",
    "\n",
    "def doc2query(doc, n=5):\n",
    "    \"\"\"Generate synthetic queries.\"\"\"\n",
    "    inputs = d2q_tokenizer(doc, max_length=512, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = d2q_model.generate(**inputs, max_length=64, do_sample=True, top_k=10, num_return_sequences=n)\n",
    "    queries = []\n",
    "    for out in outputs:\n",
    "        q = d2q_tokenizer.decode(out, skip_special_tokens=True).strip()\n",
    "        if q and q not in queries:\n",
    "            queries.append(q)\n",
    "    return queries\n",
    "\n",
    "\n",
    "def deduplicate(doc, expansions, threshold=0.85):\n",
    "    \"\"\"Remove duplicate and redundant expansions.\"\"\"\n",
    "    if not expansions:\n",
    "        return []\n",
    "    \n",
    "    # Remove duplicates among expansions\n",
    "    embs = sbert.encode(expansions, normalize_embeddings=True)\n",
    "    sim = cosine_similarity(embs)\n",
    "    kept = []\n",
    "    removed = set()\n",
    "    for i in range(len(expansions)):\n",
    "        if i not in removed:\n",
    "            kept.append(expansions[i])\n",
    "            for j in range(i+1, len(expansions)):\n",
    "                if sim[i,j] >= threshold:\n",
    "                    removed.add(j)\n",
    "    \n",
    "    # Remove expansions too similar to doc\n",
    "    if kept:\n",
    "        doc_emb = sbert.encode([doc], normalize_embeddings=True)\n",
    "        exp_embs = sbert.encode(kept, normalize_embeddings=True)\n",
    "        sims = cosine_similarity(exp_embs, doc_emb).flatten()\n",
    "        kept = [e for e, s in zip(kept, sims) if s < threshold]\n",
    "    \n",
    "    return kept[:10]\n",
    "\n",
    "\n",
    "def expand_hqfde(doc_id, doc):\n",
    "    \"\"\"Full HQF-DE pipeline.\"\"\"\n",
    "    # Phase 1: LLM expansion\n",
    "    expansions = llm_expand(doc)\n",
    "    \n",
    "    # Phase 2: NLI validation\n",
    "    valid = nli_validate(doc, expansions)\n",
    "    \n",
    "    # Phase 3: Doc2Query + combine\n",
    "    queries = doc2query(doc)\n",
    "    all_exp = valid + queries\n",
    "    final = deduplicate(doc, all_exp)\n",
    "    \n",
    "    expanded = f\"{doc} {' '.join(final)}\"\n",
    "    return doc_id, expanded\n",
    "\n",
    "\n",
    "def expand_d2q_only(doc_id, doc):\n",
    "    \"\"\"Doc2Query baseline only.\"\"\"\n",
    "    queries = doc2query(doc)\n",
    "    expanded = f\"{doc} {' '.join(queries)}\"\n",
    "    return doc_id, expanded\n",
    "\n",
    "\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test on Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "test_doc = \"The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\"\n",
    "\n",
    "print(\"Original:\", test_doc)\n",
    "print()\n",
    "\n",
    "expansions = llm_expand(test_doc)\n",
    "print(\"LLM Expansions:\")\n",
    "for e in expansions:\n",
    "    print(f\"  - {e}\")\n",
    "\n",
    "valid = nli_validate(test_doc, expansions)\n",
    "print(f\"\\nNLI Valid ({len(valid)}/{len(expansions)}):\")\n",
    "for e in valid:\n",
    "    print(f\"  + {e}\")\n",
    "\n",
    "queries = doc2query(test_doc)\n",
    "print(\"\\nDoc2Query:\")\n",
    "for q in queries:\n",
    "    print(f\"  ? {q}\")\n",
    "\n",
    "_, expanded = expand_hqfde(\"test\", test_doc)\n",
    "print(\"\\nFinal Expanded:\")\n",
    "print(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = \"/content/data/collection_subset.tsv\"\n",
    "OUTPUT_HQFDE = \"/content/output/expanded_hqfde.tsv\"\n",
    "OUTPUT_D2Q = \"/content/output/expanded_d2q.tsv\"\n",
    "LIMIT = 100000  # Process 100K docs (change to None for all 1M)\n",
    "\n",
    "# Load documents\n",
    "docs = []\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    for row in csv.reader(f, delimiter='\\t'):\n",
    "        if len(row) >= 2:\n",
    "            docs.append((row[0], row[1]))\n",
    "            if LIMIT and len(docs) >= LIMIT:\n",
    "                break\n",
    "\n",
    "print(f\"Loaded {len(docs):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HQF-DE expansion\n",
    "print(\"Running HQF-DE expansion...\")\n",
    "\n",
    "with open(OUTPUT_HQFDE, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for doc_id, text in tqdm(docs):\n",
    "        try:\n",
    "            did, expanded = expand_hqfde(doc_id, text)\n",
    "            writer.writerow([did, expanded])\n",
    "        except Exception as e:\n",
    "            writer.writerow([doc_id, text])  # Keep original on error\n",
    "\n",
    "print(f\"Saved to {OUTPUT_HQFDE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Doc2Query baseline (for comparison)\n",
    "print(\"Running Doc2Query baseline...\")\n",
    "\n",
    "with open(OUTPUT_D2Q, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for doc_id, text in tqdm(docs):\n",
    "        try:\n",
    "            did, expanded = expand_d2q_only(doc_id, text)\n",
    "            writer.writerow([did, expanded])\n",
    "        except Exception as e:\n",
    "            writer.writerow([doc_id, text])\n",
    "\n",
    "print(f\"Saved to {OUTPUT_D2Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(OUTPUT_HQFDE)\n",
    "files.download(OUTPUT_D2Q)\n",
    "\n",
    "print(\"\\nDownload complete!\")\n",
    "print(\"Next: Use these files with your C++ indexer for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
