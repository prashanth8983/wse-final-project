{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HQF-DE: Document Expansion Pipeline\n\nThis notebook expands MS MARCO documents using the HQF-DE pipeline:\n\n1. **LLM** (Llama-3-8B) → Identifies semantic gaps and generates expansions\n2. **NLI** (BART-large-mnli) → Validates expansions are factually consistent\n3. **Doc2Query** (T5) → Generates synthetic queries\n4. **Combiner** → Deduplicates and selects best expansions\n\n**Input:** `collection_100k.tsv` (100K docs, 33MB)  \n**Output:** `expanded_hqfde.tsv`, `expanded_d2q.tsv`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 20 22:52:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   36C    P0             55W /  400W |   20747MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers sentence-transformers accelerate bitsandbytes pydantic-settings sentencepiece protobuf tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace (required for Llama-3)\n# Option 1: Use Colab secrets (recommended)\n# Option 2: Paste token directly (uncomment and add token)\n\nfrom huggingface_hub import login\n\ntry:\n    from google.colab import userdata\n    token = userdata.get('HF_TOKEN')\n    login(token=token)\n    print(\"Logged in using Colab secret!\")\nexcept:\n    # For VS Code: uncomment and add your token\n    # login(token=\"your_token_here\")\n    print(\"Add your HF token: login(token='your_token_here')\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup data paths\nimport os\nos.makedirs('/content/data', exist_ok=True)\nos.makedirs('/content/output', exist_ok=True)\n\nINPUT_FILE = \"/content/data/collection_100k.tsv\"\nOUTPUT_HQFDE = \"/content/output/expanded_hqfde.tsv\"\nOUTPUT_D2Q = \"/content/output/expanded_d2q.tsv\"\n\n# Option 1: If file already uploaded to runtime\nif os.path.exists(INPUT_FILE):\n    print(f\"Found: {INPUT_FILE}\")\n    !wc -l {INPUT_FILE}\nelse:\n    # Download from Google Drive\n    # Replace FILE_ID with your file ID from the shareable link\n    FILE_ID = \"YOUR_FILE_ID_HERE\"  # Update this after uploading collection_100k.tsv\n    \n    if FILE_ID != \"YOUR_FILE_ID_HERE\":\n        !pip install -q gdown\n        import gdown\n        gdown.download(f\"https://drive.google.com/uc?id={FILE_ID}\", INPUT_FILE, quiet=False)\n    else:\n        print(\"File not found. To download from Drive:\")\n        print(\"1. Upload collection_100k.tsv to Google Drive\")\n        print(\"2. Share it (Anyone with link)\")\n        print(\"3. Copy the FILE_ID from the share link\")\n        print(\"4. Replace YOUR_FILE_ID_HERE above and re-run\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69aadf66063494dbc914e257d4c34c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeBERTa NLI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Doc2Query...\n",
      "Loading Sentence-BERT...\n",
      "All models loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "CACHE = \"/content/cache\"\n",
    "\n",
    "# Load Llama-3-8B (4-bit quantized)\n",
    "print(\"Loading Llama-3-8B...\")\n",
    "llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_name, cache_dir=CACHE)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name, cache_dir=CACHE,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_new_tokens=256)\n",
    "\n",
    "# Load NLI (DeBERTa) - using correct model name\n",
    "print(\"Loading DeBERTa NLI...\")\n",
    "nli_pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "\n",
    "# Load Doc2Query (T5)\n",
    "print(\"Loading Doc2Query...\")\n",
    "d2q_tokenizer = T5Tokenizer.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE)\n",
    "d2q_model = T5ForConditionalGeneration.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE).to(DEVICE)\n",
    "\n",
    "# Load Sentence-BERT\n",
    "print(\"Loading Sentence-BERT...\")\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "\n",
    "print(\"All models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Expansion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch functions defined!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix padding warning\n",
    "llm_tokenizer.padding_side = 'left'\n",
    "\n",
    "def llm_expand_batch(docs, batch_size=8):\n",
    "    \"\"\"Batch LLM expansion for efficiency.\"\"\"\n",
    "    all_expansions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(docs), batch_size), desc=\"  LLM\", leave=False):\n",
    "        batch = docs[i:i+batch_size]\n",
    "        prompts = []\n",
    "        for doc in batch:\n",
    "            prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Analyze this document and generate 3-5 brief factual expansions that add missing context:\n",
    "{doc}\n",
    "\n",
    "Expansions:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        results = llm_pipe(prompts, return_full_text=False, pad_token_id=llm_tokenizer.pad_token_id, batch_size=len(prompts))\n",
    "        \n",
    "        for result in results:\n",
    "            text = result[0][\"generated_text\"]\n",
    "            expansions = []\n",
    "            for line in text.split(\"\\n\"):\n",
    "                line = line.strip()\n",
    "                if line and len(line) > 10:\n",
    "                    if line[0].isdigit():\n",
    "                        line = line.split(\".\", 1)[-1].strip()\n",
    "                    if line.startswith(\"-\"):\n",
    "                        line = line[1:].strip()\n",
    "                    if line:\n",
    "                        expansions.append(line)\n",
    "            all_expansions.append(expansions[:5])\n",
    "    \n",
    "    return all_expansions\n",
    "\n",
    "\n",
    "def nli_validate_batch(docs, expansions_list, threshold=0.7):\n",
    "    \"\"\"Batch NLI validation.\"\"\"\n",
    "    # Flatten all (doc, expansion) pairs\n",
    "    pairs = []\n",
    "    pair_indices = []\n",
    "    for i, (doc, expansions) in enumerate(zip(docs, expansions_list)):\n",
    "        for exp in expansions:\n",
    "            pairs.append((doc, exp))\n",
    "            pair_indices.append(i)\n",
    "    \n",
    "    if not pairs:\n",
    "        return [[] for _ in docs]\n",
    "    \n",
    "    # Process in batches\n",
    "    valid_flags = []\n",
    "    batch_size = 32\n",
    "    for i in tqdm(range(0, len(pairs), batch_size), desc=\"  NLI\", leave=False):\n",
    "        batch = pairs[i:i+batch_size]\n",
    "        try:\n",
    "            results = nli_pipe(\n",
    "                [exp for _, exp in batch],\n",
    "                candidate_labels=[\"entailed\", \"not entailed\"],\n",
    "                batch_size=len(batch)\n",
    "            )\n",
    "            if not isinstance(results, list):\n",
    "                results = [results]\n",
    "            for r in results:\n",
    "                is_valid = r['labels'][0] == 'entailed' and r['scores'][0] >= threshold\n",
    "                valid_flags.append(is_valid)\n",
    "        except:\n",
    "            valid_flags.extend([False] * len(batch))\n",
    "    \n",
    "    # Reconstruct per-document valid lists\n",
    "    all_valid = [[] for _ in docs]\n",
    "    for idx, ((doc, exp), is_valid) in enumerate(zip(pairs, valid_flags)):\n",
    "        if is_valid:\n",
    "            all_valid[pair_indices[idx]].append(exp)\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "\n",
    "def doc2query_batch(docs, n=5, batch_size=16):\n",
    "    \"\"\"Batch Doc2Query generation.\"\"\"\n",
    "    all_queries = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(docs), batch_size), desc=\"  D2Q\", leave=False):\n",
    "        batch = docs[i:i+batch_size]\n",
    "        inputs = d2q_tokenizer(batch, max_length=512, truncation=True, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = d2q_model.generate(\n",
    "                **inputs, \n",
    "                max_length=64, \n",
    "                do_sample=True, \n",
    "                top_k=10, \n",
    "                num_return_sequences=n\n",
    "            )\n",
    "        \n",
    "        # Reshape: (batch_size * n) -> (batch_size, n)\n",
    "        for j in range(len(batch)):\n",
    "            queries = []\n",
    "            for k in range(n):\n",
    "                idx = j * n + k\n",
    "                if idx < len(outputs):\n",
    "                    q = d2q_tokenizer.decode(outputs[idx], skip_special_tokens=True).strip()\n",
    "                    if q and q not in queries:\n",
    "                        queries.append(q)\n",
    "            all_queries.append(queries)\n",
    "    \n",
    "    return all_queries\n",
    "\n",
    "\n",
    "def deduplicate_batch(docs, expansions_list, threshold=0.85):\n",
    "    \"\"\"Batch deduplication using embeddings.\"\"\"\n",
    "    all_final = []\n",
    "    \n",
    "    for doc, expansions in zip(docs, expansions_list):\n",
    "        if not expansions:\n",
    "            all_final.append([])\n",
    "            continue\n",
    "        \n",
    "        # Remove duplicates among expansions\n",
    "        embs = sbert.encode(expansions, normalize_embeddings=True)\n",
    "        sim = cosine_similarity(embs)\n",
    "        kept = []\n",
    "        removed = set()\n",
    "        for i in range(len(expansions)):\n",
    "            if i not in removed:\n",
    "                kept.append(expansions[i])\n",
    "                for j in range(i+1, len(expansions)):\n",
    "                    if sim[i,j] >= threshold:\n",
    "                        removed.add(j)\n",
    "        \n",
    "        # Remove expansions too similar to doc\n",
    "        if kept:\n",
    "            doc_emb = sbert.encode([doc], normalize_embeddings=True)\n",
    "            exp_embs = sbert.encode(kept, normalize_embeddings=True)\n",
    "            sims = cosine_similarity(exp_embs, doc_emb).flatten()\n",
    "            kept = [e for e, s in zip(kept, sims) if s < threshold]\n",
    "        \n",
    "        all_final.append(kept[:10])\n",
    "    \n",
    "    return all_final\n",
    "\n",
    "\n",
    "def expand_batch(doc_ids, docs, batch_size=8):\n",
    "    \"\"\"Full HQF-DE pipeline with batching.\"\"\"\n",
    "    print(f\"    Processing {len(docs)} docs...\")\n",
    "    \n",
    "    # Phase 1: LLM expansion\n",
    "    llm_expansions = llm_expand_batch(docs, batch_size=batch_size)\n",
    "    \n",
    "    # Phase 2: NLI validation\n",
    "    valid_expansions = nli_validate_batch(docs, llm_expansions)\n",
    "    \n",
    "    # Phase 3: Doc2Query\n",
    "    queries = doc2query_batch(docs, n=5, batch_size=batch_size*2)\n",
    "    \n",
    "    # Phase 4: Combine and deduplicate\n",
    "    combined = [v + q for v, q in zip(valid_expansions, queries)]\n",
    "    final = deduplicate_batch(docs, combined)\n",
    "    \n",
    "    # Build results\n",
    "    results = []\n",
    "    for doc_id, doc, f in zip(doc_ids, docs, final):\n",
    "        expanded = f\"{doc} {' '.join(f)}\"\n",
    "        results.append((doc_id, expanded))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_d2q_batch(doc_ids, docs, batch_size=16):\n",
    "    \"\"\"Doc2Query baseline with batching.\"\"\"\n",
    "    queries = doc2query_batch(docs, n=5, batch_size=batch_size)\n",
    "    \n",
    "    results = []\n",
    "    for doc_id, doc, q in zip(doc_ids, docs, queries):\n",
    "        expanded = f\"{doc} {' '.join(q)}\"\n",
    "        results.append((doc_id, expanded))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Batch functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test on Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\n",
      "\n",
      "    Processing 1 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded:\n",
      "The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889. Based on the provided document, I've generated the following brief factual expansions that add missing context: **The Eiffel Tower was built for the World's Fair**: The Eiffel Tower was constructed for the 1889 World's Fair, held in Paris, France. The fair was a celebration of French culture, technology, and innovation, and the tower was the main centerpiece of the event. **Designed by Gustave Eiffel**: The Eiffel Tower was designed and built by Gustave Eiffel, a French engineer and entrepreneur. Eiffel's company, Compagnie des Établissements Eiffel, was responsible for the tower's construction. eiffel tower famous buildings\n",
      "\n",
      "Expansion added: Based on the provided document, I've generated the following brief factual expansions that add missing context: **The Eiffel Tower was built for the World's Fair**: The Eiffel Tower was constructed for the 1889 World's Fair, held in Paris, France. The fair was a celebration of French culture, technology, and innovation, and the tower was the main centerpiece of the event. **Designed by Gustave Eiffel**: The Eiffel Tower was designed and built by Gustave Eiffel, a French engineer and entrepreneur. Eiffel's company, Compagnie des Établissements Eiffel, was responsible for the tower's construction. eiffel tower famous buildings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Test on a sample document\n",
    "test_doc = \"The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\"\n",
    "test_id = \"test\"\n",
    "\n",
    "print(\"Original:\", test_doc)\n",
    "print()\n",
    "\n",
    "# Test batch functions with single doc\n",
    "results = expand_batch([test_id], [test_doc], batch_size=1)\n",
    "doc_id, expanded = results[0]\n",
    "\n",
    "print(\"Expanded:\")\n",
    "print(expanded)\n",
    "print()\n",
    "print(\"Expansion added:\", expanded.replace(test_doc, \"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import csv\nfrom tqdm import tqdm\n\nLIMIT = None  # Process all 100K docs (set to 1000 for testing)\nBATCH_SIZE = 32  # Adjust based on GPU memory\nCHECKPOINT_EVERY = 5  # Save progress every N batches (smaller = more updates)\n\n# Load documents\ndocs = []\ndoc_ids = []\nwith open(INPUT_FILE, 'r') as f:\n    for row in csv.reader(f, delimiter='\\t'):\n        if len(row) >= 2:\n            doc_ids.append(row[0])\n            docs.append(row[1])\n            if LIMIT and len(docs) >= LIMIT:\n                break\n\nprint(f\"Loaded {len(docs):,} documents\")\nprint(f\"Will process in mega-batches of {BATCH_SIZE * CHECKPOINT_EVERY:,} docs\")\nprint(f\"Estimated time at 0.6 docs/sec: {len(docs)/0.6/3600:.1f} hours\")"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HQF-DE expansion on 1,000 docs with batch_size=32...\n",
      "Output: /content/output/expanded_hqfde.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing 160 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:  14%|█▍        | 1/7 [04:05<24:35, 245.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed: 160 | Rate: 0.7 docs/sec | ETA: 0.4 hrs\n",
      "    Processing 160 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:  29%|██▊       | 2/7 [09:06<23:10, 278.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing 160 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:  43%|████▎     | 3/7 [13:13<17:35, 263.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing 160 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:  57%|█████▋    | 4/7 [17:41<13:16, 265.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing 160 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:  71%|███████▏  | 5/7 [22:13<08:55, 267.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing 160 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE:  86%|████████▌ | 6/7 [26:46<04:29, 269.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing 40 docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HQF-DE: 100%|██████████| 7/7 [28:23<00:00, 243.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Processed 1,000 docs in 0.47 hours\n",
      "Rate: 0.6 docs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run HQF-DE expansion with batching\n",
    "import time\n",
    "print(f\"Running HQF-DE expansion on {len(docs):,} docs with batch_size={BATCH_SIZE}...\")\n",
    "print(f\"Output: {OUTPUT_HQFDE}\")\n",
    "\n",
    "start_time = time.time()\n",
    "processed = 0\n",
    "mega_batch = BATCH_SIZE * CHECKPOINT_EVERY  # Process this many before checkpoint\n",
    "\n",
    "with open(OUTPUT_HQFDE, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    \n",
    "    for i in tqdm(range(0, len(docs), mega_batch), desc=\"HQF-DE\"):\n",
    "        batch_doc_ids = doc_ids[i:i+mega_batch]\n",
    "        batch_docs = docs[i:i+mega_batch]\n",
    "        \n",
    "        try:\n",
    "            results = expand_batch(batch_doc_ids, batch_docs, batch_size=BATCH_SIZE)\n",
    "            for doc_id, expanded in results:\n",
    "                writer.writerow([doc_id, expanded])\n",
    "            f.flush()  # Ensure data is written\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            # Write originals on error\n",
    "            for doc_id, doc in zip(batch_doc_ids, batch_docs):\n",
    "                writer.writerow([doc_id, doc])\n",
    "        \n",
    "        processed += len(batch_docs)\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = processed / elapsed\n",
    "        remaining = (len(docs) - processed) / rate if rate > 0 else 0\n",
    "        \n",
    "        if (i // mega_batch) % 10 == 0:\n",
    "            print(f\"  Processed: {processed:,} | Rate: {rate:.1f} docs/sec | ETA: {remaining/3600:.1f} hrs\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nDone! Processed {processed:,} docs in {total_time/3600:.2f} hours\")\n",
    "print(f\"Rate: {processed/total_time:.1f} docs/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Doc2Query baseline on 1,000 docs...\n",
      "Output: /content/output/expanded_d2q.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doc2Query: 100%|██████████| 7/7 [00:27<00:00,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Processed 1,000 docs in 0.01 hours\n",
      "Rate: 36.0 docs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Doc2Query baseline with batching (much faster)\n",
    "import time\n",
    "print(f\"Running Doc2Query baseline on {len(docs):,} docs...\")\n",
    "print(f\"Output: {OUTPUT_D2Q}\")\n",
    "\n",
    "start_time = time.time()\n",
    "D2Q_BATCH = 32  # D2Q can handle larger batches\n",
    "\n",
    "with open(OUTPUT_D2Q, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    \n",
    "    for i in tqdm(range(0, len(docs), mega_batch), desc=\"Doc2Query\"):\n",
    "        batch_doc_ids = doc_ids[i:i+mega_batch]\n",
    "        batch_docs = docs[i:i+mega_batch]\n",
    "        \n",
    "        try:\n",
    "            results = expand_d2q_batch(batch_doc_ids, batch_docs, batch_size=D2Q_BATCH)\n",
    "            for doc_id, expanded in results:\n",
    "                writer.writerow([doc_id, expanded])\n",
    "            f.flush()\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            for doc_id, doc in zip(batch_doc_ids, batch_docs):\n",
    "                writer.writerow([doc_id, doc])\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nDone! Processed {len(docs):,} docs in {total_time/3600:.2f} hours\")\n",
    "print(f\"Rate: {len(docs)/total_time:.1f} docs/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files saved to Google Drive:\n",
      "  HQF-DE: /content/output/expanded_hqfde.tsv (1.7 MB)\n",
      "  D2Q:    /content/output/expanded_d2q.tsv (0.5 MB)\n",
      "\n",
      "Files are automatically synced to your Google Drive.\n",
      "Next: Use these with your C++ indexer for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Output files saved to Google Drive:\")\n",
    "print(f\"  HQF-DE: {OUTPUT_HQFDE} ({os.path.getsize(OUTPUT_HQFDE)/1e6:.1f} MB)\")\n",
    "print(f\"  D2Q:    {OUTPUT_D2Q} ({os.path.getsize(OUTPUT_D2Q)/1e6:.1f} MB)\")\n",
    "print()\n",
    "print(\"Files are automatically synced to your Google Drive.\")\n",
    "print(\"Next: Use these with your C++ indexer for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading output files...\n",
      "\n",
      "expanded_hqfde.tsv (1.7 MB)\n",
      "  transfer.sh failed: HTTPSConnectionPool(host='transfer.sh', port=443): Max retries exceeded with url: /expanded_hqfde.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x78819008d490>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "  ✓ http://tmpfiles.org/dl/16360996/expanded_hqfde.tsv\n",
      "\n",
      "expanded_d2q.tsv (0.5 MB)\n",
      "  transfer.sh failed: HTTPSConnectionPool(host='transfer.sh', port=443): Max retries exceeded with url: /expanded_d2q.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x78819008f230>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "  ✓ http://tmpfiles.org/dl/16361000/expanded_d2q.tsv\n",
      "\n",
      "Download these files using the URLs above.\n"
     ]
    }
   ],
   "source": [
    "# Upload output files to get download links (VS Code compatible)\n",
    "!pip install -q requests\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "print(\"Uploading output files...\\n\")\n",
    "\n",
    "for filename in ['expanded_hqfde.tsv', 'expanded_d2q.tsv']:\n",
    "    filepath = f'/content/output/{filename}'\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / 1e6\n",
    "        print(f\"{filename} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        uploaded = False\n",
    "        \n",
    "        # Try 0x0.st\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                response = requests.post('https://0x0.st', files={'file': f}, timeout=60)\n",
    "            if response.status_code == 200 and response.text.startswith('http'):\n",
    "                print(f\"  ✓ {response.text.strip()}\")\n",
    "                uploaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"  0x0.st failed: {e}\")\n",
    "        \n",
    "        # Try transfer.sh\n",
    "        if not uploaded:\n",
    "            try:\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    response = requests.put(f'https://transfer.sh/{filename}', data=f, timeout=60)\n",
    "                if response.status_code == 200 and response.text.startswith('http'):\n",
    "                    print(f\"  ✓ {response.text.strip()}\")\n",
    "                    uploaded = True\n",
    "            except Exception as e:\n",
    "                print(f\"  transfer.sh failed: {e}\")\n",
    "        \n",
    "        # Try tmpfiles.org\n",
    "        if not uploaded:\n",
    "            try:\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    response = requests.post('https://tmpfiles.org/api/v1/upload', files={'file': f}, timeout=60)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    if data.get('status') == 'success':\n",
    "                        # Convert URL from tmpfiles.org/123/file to tmpfiles.org/dl/123/file\n",
    "                        url = data['data']['url'].replace('tmpfiles.org/', 'tmpfiles.org/dl/')\n",
    "                        print(f\"  ✓ {url}\")\n",
    "                        uploaded = True\n",
    "            except Exception as e:\n",
    "                print(f\"  tmpfiles.org failed: {e}\")\n",
    "        \n",
    "        if not uploaded:\n",
    "            print(f\"  ✗ All upload methods failed\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"✗ {filepath} not found\\n\")\n",
    "\n",
    "print(\"Download these files using the URLs above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}