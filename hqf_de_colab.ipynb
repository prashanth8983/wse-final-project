{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HQF-DE: Hybrid Query- and Fact-Guided Document Expansion\n",
    "\n",
    "This notebook runs the HQF-DE pipeline on Google Colab with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers sentence-transformers scikit-learn pydantic-settings sentencepiece protobuf accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace (required for Llama access)\n# Option 1: Use Colab secrets (recommended)\n# from google.colab import userdata\n# from huggingface_hub import login\n# login(token=userdata.get('HF_TOKEN'))\n\n# Option 2: Enter token manually when prompted\nfrom huggingface_hub import login\nlogin()  # Will prompt for token"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1631552359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive (optional - for saving results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (optional - for saving results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or upload the project\n",
    "# Option 1: If uploaded to Drive\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/wse-final-project')\n",
    "\n",
    "# Option 2: Clone from GitHub (if you push it)\n",
    "# !git clone https://github.com/yourusername/wse-final-project.git\n",
    "# %cd wse-final-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hqf_de package inline (if not using Drive/GitHub)\n",
    "!mkdir -p hqf_de/models hqf_de/pipeline hqf_de/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/config.py\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field\n",
    "from pathlib import Path\n",
    "\n",
    "class HQFDEConfig(BaseSettings):\n",
    "    project_root: Path = Field(default=Path(\"/content\"))\n",
    "    data_dir: Path = Field(default=Path(\"/content/data\"))\n",
    "    output_dir: Path = Field(default=Path(\"/content/output\"))\n",
    "    cache_dir: Path = Field(default=Path(\"/content/cache\"))\n",
    "\n",
    "    input_tsv: str = Field(default=\"collection.tsv\")\n",
    "    output_tsv: str = Field(default=\"expanded_passages.tsv\")\n",
    "\n",
    "    llm_model_name: str = Field(default=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    llm_max_new_tokens: int = Field(default=256)\n",
    "    llm_temperature: float = Field(default=0.7)\n",
    "\n",
    "    doc2query_model: str = Field(default=\"castorini/doc2query-t5-base-msmarco\")\n",
    "    num_queries_per_doc: int = Field(default=5)\n",
    "\n",
    "    nli_model: str = Field(default=\"microsoft/deberta-v3-large-mnli\")\n",
    "    nli_entailment_threshold: float = Field(default=0.9)\n",
    "\n",
    "    embedding_model: str = Field(default=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    dedup_similarity_threshold: float = Field(default=0.85)\n",
    "\n",
    "    batch_size: int = Field(default=8)\n",
    "    max_doc_length: int = Field(default=512)\n",
    "    device: str = Field(default=\"cuda\")\n",
    "\n",
    "    class Config:\n",
    "        env_prefix = \"HQFDE_\"\n",
    "\n",
    "config = HQFDEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/__init__.py\n",
    "__version__ = \"0.1.0\"\n",
    "from .config import config, HQFDEConfig\n",
    "__all__ = [\"config\", \"HQFDEConfig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/__init__.py\n",
    "from .llm import LLM\n",
    "from .doc2query import Doc2Query\n",
    "from .nli import NLI\n",
    "from .embeddings import Embedder\n",
    "__all__ = [\"LLM\", \"Doc2Query\", \"NLI\", \"Embedder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/llm.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Expansion:\n",
    "    text: str\n",
    "    gaps: List[str]\n",
    "    expansions: List[str]\n",
    "\n",
    "class LLM:\n",
    "    GAP_PROMPT = \"\"\"Analyze this document and list semantic gaps (max 5):\n",
    "{document}\n",
    "\n",
    "Gaps:\"\"\"\n",
    "\n",
    "    EXPAND_PROMPT = \"\"\"Generate brief factual expansions for this document:\n",
    "{document}\n",
    "\n",
    "Gaps: {gaps}\n",
    "\n",
    "Expansions:\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = None, device: str = None, quantize: bool = True):\n",
    "        self.model_name = model or config.llm_model_name\n",
    "        self.device = device or config.device\n",
    "        self.quantize = quantize\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipe = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading LLM: {self.model_name}\")\n",
    "        quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16) if self.quantize else None\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir=config.cache_dir)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, cache_dir=config.cache_dir, quantization_config=quant_config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer, max_new_tokens=config.llm_max_new_tokens, temperature=config.llm_temperature, do_sample=True)\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def _fmt(self, doc: str, template: str, **kw) -> str:\n",
    "        content = template.format(document=doc, **kw)\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    def _gen(self, prompt: str) -> str:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        result = self.pipe(prompt, return_full_text=False, pad_token_id=self.tokenizer.pad_token_id)\n",
    "        return result[0][\"generated_text\"].strip()\n",
    "\n",
    "    def _parse(self, text: str) -> List[str]:\n",
    "        items = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\"#\"):\n",
    "                if line[0].isdigit():\n",
    "                    line = line.split(\".\", 1)[-1].strip()\n",
    "                if line.startswith(\"-\"):\n",
    "                    line = line[1:].strip()\n",
    "                if line and len(line) > 5:\n",
    "                    items.append(line)\n",
    "        return items[:5]\n",
    "\n",
    "    def gaps(self, doc: str) -> List[str]:\n",
    "        return self._parse(self._gen(self._fmt(doc, self.GAP_PROMPT)))\n",
    "\n",
    "    def expand(self, doc: str, gaps: List[str] = None) -> List[str]:\n",
    "        gaps_text = \"\\n\".join(gaps) if gaps else \"none\"\n",
    "        return self._parse(self._gen(self._fmt(doc, self.EXPAND_PROMPT, gaps=gaps_text)))\n",
    "\n",
    "    def run(self, doc: str) -> Expansion:\n",
    "        g = self.gaps(doc)\n",
    "        e = self.expand(doc, g)\n",
    "        return Expansion(text=doc, gaps=g, expansions=e)\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model, self.tokenizer, self.pipe\n",
    "            self.model = self.tokenizer = self.pipe = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/doc2query.py\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from typing import List\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Doc2Query:\n",
    "    def __init__(self, model: str = None, device: str = None, n: int = 5):\n",
    "        self.model_name = model or config.doc2query_model\n",
    "        self.device = device or config.device\n",
    "        self.n = n\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading doc2query: {self.model_name}\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, cache_dir=config.cache_dir)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name, cache_dir=config.cache_dir).to(self.device)\n",
    "        self.model.eval()\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def gen(self, doc: str, n: int = None) -> List[str]:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        n = n or self.n\n",
    "        inputs = self.tokenizer(doc, max_length=config.max_doc_length, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_length=64, do_sample=True, top_k=10, num_return_sequences=n)\n",
    "        queries = []\n",
    "        for out in outputs:\n",
    "            q = self.tokenizer.decode(out, skip_special_tokens=True).strip()\n",
    "            if q and q not in queries:\n",
    "                queries.append(q)\n",
    "        return queries\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model, self.tokenizer\n",
    "            self.model = self.tokenizer = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/nli.py\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class NLIResult:\n",
    "    hypothesis: str\n",
    "    entailment: float\n",
    "    valid: bool\n",
    "\n",
    "class NLI:\n",
    "    def __init__(self, model: str = None, device: str = None, threshold: float = 0.9):\n",
    "        self.model_name = model or config.nli_model\n",
    "        self.device = device or config.device\n",
    "        self.threshold = threshold\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipe = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading NLI: {self.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir=config.cache_dir)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, cache_dir=config.cache_dir).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.pipe = pipeline(\"text-classification\", model=self.model, tokenizer=self.tokenizer, device=0, top_k=None)\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def check(self, premise: str, hypothesis: str) -> NLIResult:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        try:\n",
    "            results = self.pipe(f\"{premise} [SEP] {hypothesis}\", truncation=True, max_length=512)\n",
    "            scores = {r[\"label\"].lower(): r[\"score\"] for r in results}\n",
    "            ent = scores.get(\"entailment\", 0.0)\n",
    "            return NLIResult(hypothesis=hypothesis, entailment=ent, valid=ent >= self.threshold)\n",
    "        except:\n",
    "            return NLIResult(hypothesis=hypothesis, entailment=0.0, valid=False)\n",
    "\n",
    "    def validate(self, doc: str, expansions: List[str]) -> Tuple[List[str], List[NLIResult]]:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        valid = []\n",
    "        results = []\n",
    "        for exp in expansions:\n",
    "            r = self.check(doc, exp)\n",
    "            results.append(r)\n",
    "            if r.valid:\n",
    "                valid.append(exp)\n",
    "        return valid, results\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model, self.tokenizer, self.pipe\n",
    "            self.model = self.tokenizer = self.pipe = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/embeddings.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model: str = None, device: str = None):\n",
    "        self.model_name = model or config.embedding_model\n",
    "        self.device = device or config.device\n",
    "        self.model = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading embedder: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name, cache_folder=str(config.cache_dir), device=self.device)\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        return self.model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "    def sim(self, texts1: List[str], texts2: List[str] = None) -> np.ndarray:\n",
    "        e1 = self.encode(texts1)\n",
    "        if texts2 is None:\n",
    "            return cosine_similarity(e1)\n",
    "        return cosine_similarity(e1, self.encode(texts2))\n",
    "\n",
    "    def dedup(self, texts: List[str], threshold: float = 0.85) -> Tuple[List[str], List[int]]:\n",
    "        if len(texts) <= 1:\n",
    "            return texts, list(range(len(texts)))\n",
    "        sim = self.sim(texts)\n",
    "        kept, indices, removed = [], [], set()\n",
    "        for i in range(len(texts)):\n",
    "            if i in removed:\n",
    "                continue\n",
    "            kept.append(texts[i])\n",
    "            indices.append(i)\n",
    "            for j in range(i + 1, len(texts)):\n",
    "                if sim[i, j] >= threshold:\n",
    "                    removed.add(j)\n",
    "        return kept, indices\n",
    "\n",
    "    def dedup_vs_doc(self, doc: str, expansions: List[str], threshold: float = 0.85) -> List[str]:\n",
    "        if not expansions:\n",
    "            return []\n",
    "        sims = self.sim(expansions, [doc]).flatten()\n",
    "        return [e for e, s in zip(expansions, sims) if s < threshold]\n",
    "\n",
    "    def select(self, expansions: List[str], n: int = 5, doc: str = None) -> List[str]:\n",
    "        if len(expansions) <= n:\n",
    "            return expansions\n",
    "        embs = self.encode(expansions)\n",
    "        rel = self.sim(expansions, [doc]).flatten() if doc else np.ones(len(expansions))\n",
    "        selected, indices = [], []\n",
    "        for _ in range(n):\n",
    "            best_idx, best_score = -1, -float('inf')\n",
    "            for i in range(len(expansions)):\n",
    "                if i in indices:\n",
    "                    continue\n",
    "                div = 1 - max(cosine_similarity(embs[i:i+1], embs[indices]).flatten()) if indices else 1.0\n",
    "                score = 0.5 * rel[i] + 0.5 * div\n",
    "                if score > best_score:\n",
    "                    best_score, best_idx = score, i\n",
    "            if best_idx >= 0:\n",
    "                selected.append(expansions[best_idx])\n",
    "                indices.append(best_idx)\n",
    "        return selected\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/pipeline/__init__.py\n",
    "from .expander import Expander\n",
    "from .combiner import Combiner\n",
    "__all__ = [\"Expander\", \"Combiner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/pipeline/combiner.py\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from ..models.embeddings import Embedder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "GENERIC = {\"information\", \"details\", \"things\", \"stuff\", \"content\", \"topic\", \"subject\", \"matter\", \"example\", \"case\", \"way\", \"method\", \"people\", \"time\", \"place\", \"thing\"}\n",
    "\n",
    "@dataclass\n",
    "class Combined:\n",
    "    original: str\n",
    "    semantic: List[str] = field(default_factory=list)\n",
    "    queries: List[str] = field(default_factory=list)\n",
    "    final: List[str] = field(default_factory=list)\n",
    "    text: str = \"\"\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class Combiner:\n",
    "    def __init__(self, embedder: Embedder = None, threshold: float = 0.85, max_exp: int = 10):\n",
    "        self.embedder = embedder or Embedder()\n",
    "        self.threshold = threshold\n",
    "        self.max_exp = max_exp\n",
    "        self._loaded = False\n",
    "\n",
    "    def _load(self):\n",
    "        if not self._loaded:\n",
    "            self.embedder.load()\n",
    "            self._loaded = True\n",
    "\n",
    "    def _filter(self, expansions: List[str], doc: str = None) -> List[str]:\n",
    "        out = []\n",
    "        for e in expansions:\n",
    "            e = e.strip()\n",
    "            if not e or len(e.split()) < 3 or len(e.split()) > 50:\n",
    "                continue\n",
    "            words = e.lower().split()\n",
    "            if sum(1 for w in words if w in GENERIC) / len(words) > 0.5:\n",
    "                continue\n",
    "            if doc and e.lower() in doc.lower():\n",
    "                continue\n",
    "            out.append(e)\n",
    "        return out\n",
    "\n",
    "    def _dedup(self, expansions: List[str], doc: str = None) -> List[str]:\n",
    "        if len(expansions) <= 1:\n",
    "            return expansions\n",
    "        self._load()\n",
    "        deduped, _ = self.embedder.dedup(expansions, self.threshold)\n",
    "        if doc:\n",
    "            deduped = self.embedder.dedup_vs_doc(doc, deduped, self.threshold)\n",
    "        return deduped\n",
    "\n",
    "    def combine(self, doc: str, semantic: List[str], queries: List[str]) -> Combined:\n",
    "        self._load()\n",
    "        sem = self._filter(semantic, doc)\n",
    "        q = self._filter(queries, doc)\n",
    "        all_exp = sem + q\n",
    "        deduped = self._dedup(all_exp, doc)\n",
    "        final = self.embedder.select(deduped, self.max_exp, doc) if len(deduped) > self.max_exp else deduped[:self.max_exp]\n",
    "        text = f\"{doc} {' '.join(final)}\"\n",
    "        return Combined(original=doc, semantic=sem, queries=q, final=final, text=text, meta={\"n_sem\": len(sem), \"n_q\": len(q), \"n_final\": len(final)})\n",
    "\n",
    "    def unload(self):\n",
    "        if self._loaded:\n",
    "            self.embedder.unload()\n",
    "            self._loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/pipeline/expander.py\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from ..models.llm import LLM\n",
    "from ..models.nli import NLI\n",
    "from ..models.doc2query import Doc2Query\n",
    "from ..models.embeddings import Embedder\n",
    "from .combiner import Combiner\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    doc_id: str\n",
    "    original: str\n",
    "    expanded: str\n",
    "    gaps: List[str] = field(default_factory=list)\n",
    "    raw: List[str] = field(default_factory=list)\n",
    "    valid: List[str] = field(default_factory=list)\n",
    "    rejected: List[str] = field(default_factory=list)\n",
    "    queries: List[str] = field(default_factory=list)\n",
    "    final: List[str] = field(default_factory=list)\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class Expander:\n",
    "    def __init__(self, use_llm: bool = True, use_nli: bool = True, use_d2q: bool = True, device: str = None):\n",
    "        self.device = device or config.device\n",
    "        self.llm = LLM(device=self.device) if use_llm else None\n",
    "        self.nli = NLI(device=self.device) if use_nli else None\n",
    "        self.d2q = Doc2Query(device=self.device) if use_d2q else None\n",
    "        self.combiner = Combiner(Embedder(device=self.device))\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        if self.llm:\n",
    "            self.llm.load()\n",
    "        if self.nli:\n",
    "            self.nli.load()\n",
    "        if self.d2q:\n",
    "            self.d2q.load()\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def expand(self, doc_id: str, doc: str) -> Result:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        result = Result(doc_id=doc_id, original=doc, expanded=doc)\n",
    "        raw = []\n",
    "        if self.llm:\n",
    "            try:\n",
    "                exp = self.llm.run(doc)\n",
    "                result.gaps = exp.gaps\n",
    "                raw = exp.expansions\n",
    "                result.raw = raw\n",
    "            except Exception as e:\n",
    "                logger.error(f\"LLM error: {e}\")\n",
    "        valid = raw\n",
    "        if self.nli and raw:\n",
    "            try:\n",
    "                valid, _ = self.nli.validate(doc, raw)\n",
    "                result.valid = valid\n",
    "                result.rejected = [e for e in raw if e not in valid]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"NLI error: {e}\")\n",
    "        queries = []\n",
    "        if self.d2q:\n",
    "            try:\n",
    "                queries = self.d2q.gen(doc)\n",
    "                result.queries = queries\n",
    "            except Exception as e:\n",
    "                logger.error(f\"D2Q error: {e}\")\n",
    "        try:\n",
    "            combined = self.combiner.combine(doc, valid, queries)\n",
    "            result.final = combined.final\n",
    "            result.expanded = combined.text\n",
    "            result.meta = combined.meta\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Combiner error: {e}\")\n",
    "            all_exp = valid + queries\n",
    "            result.final = all_exp[:10]\n",
    "            result.expanded = f\"{doc} {' '.join(all_exp[:10])}\"\n",
    "        return result\n",
    "\n",
    "    def d2q_only(self, doc_id: str, doc: str) -> Result:\n",
    "        if self.d2q and not self.d2q._loaded:\n",
    "            self.d2q.load()\n",
    "        queries = self.d2q.gen(doc) if self.d2q else []\n",
    "        return Result(doc_id=doc_id, original=doc, expanded=f\"{doc} {' '.join(queries)}\", queries=queries, final=queries, meta={\"method\": \"d2q_only\"})\n",
    "\n",
    "    def unload(self):\n",
    "        if self.llm:\n",
    "            self.llm.unload()\n",
    "        if self.nli:\n",
    "            self.nli.unload()\n",
    "        if self.d2q:\n",
    "            self.d2q.unload()\n",
    "        self.combiner.unload()\n",
    "        self._loaded = False\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.load()\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/evaluation/__init__.py\n",
    "# Evaluation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data and output directories\n",
    "!mkdir -p /content/data /content/output /content/cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run HQF-DE Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from hqf_de.pipeline.expander import Expander\n",
    "\n",
    "# Initialize expander\n",
    "exp = Expander()\n",
    "exp.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on a sample document\n",
    "doc = \"The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\"\n",
    "\n",
    "result = exp.expand(\"demo\", doc)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL:\")\n",
    "print(result.original)\n",
    "print()\n",
    "print(\"SEMANTIC GAPS:\")\n",
    "for g in result.gaps:\n",
    "    print(f\"  - {g}\")\n",
    "print()\n",
    "print(\"VALID EXPANSIONS (passed NLI):\")\n",
    "for e in result.valid:\n",
    "    print(f\"  + {e}\")\n",
    "print()\n",
    "print(\"REJECTED (failed NLI):\")\n",
    "for e in result.rejected:\n",
    "    print(f\"  x {e}\")\n",
    "print()\n",
    "print(\"SYNTHETIC QUERIES:\")\n",
    "for q in result.queries:\n",
    "    print(f\"  ? {q}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL EXPANDED:\")\n",
    "print(result.expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "exp.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload collection.tsv from local machine\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Select your collection.tsv file\n",
    "\n",
    "# Move to data directory\n",
    "import shutil\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, f'/content/data/{filename}')\n",
    "    print(f\"Moved {filename} to /content/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch expand documents and save to TSV\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_docs(path, limit=None):\n",
    "    docs = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for row in csv.reader(f, delimiter='\\t'):\n",
    "            if len(row) >= 2:\n",
    "                docs.append((row[0], row[1]))\n",
    "                if limit and len(docs) >= limit:\n",
    "                    break\n",
    "    return docs\n",
    "\n",
    "def save_expanded(results, path):\n",
    "    with open(path, 'w', encoding='utf-8', newline='') as f:\n",
    "        w = csv.writer(f, delimiter='\\t')\n",
    "        for r in results:\n",
    "            w.writerow([r.doc_id, r.expanded])\n",
    "    print(f\"Saved {len(results)} documents to {path}\")\n",
    "\n",
    "# Load documents\n",
    "LIMIT = 1000  # Adjust as needed\n",
    "docs = load_docs('/content/data/collection.tsv', limit=LIMIT)\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Expand with full HQF-DE pipeline\n",
    "exp = Expander()\n",
    "exp.load()\n",
    "\n",
    "results = []\n",
    "for doc_id, text in tqdm(docs, desc=\"Expanding\"):\n",
    "    result = exp.expand(doc_id, text)\n",
    "    results.append(result)\n",
    "\n",
    "exp.unload()\n",
    "\n",
    "# Save results\n",
    "save_expanded(results, '/content/output/expanded_hqfde.tsv')\n",
    "\n",
    "# Also save d2q-only baseline for comparison\n",
    "exp_d2q = Expander(use_llm=False, use_nli=False, use_d2q=True)\n",
    "exp_d2q.load()\n",
    "\n",
    "results_d2q = []\n",
    "for doc_id, text in tqdm(docs, desc=\"D2Q only\"):\n",
    "    result = exp_d2q.d2q_only(doc_id, text)\n",
    "    results_d2q.append(result)\n",
    "\n",
    "exp_d2q.unload()\n",
    "\n",
    "save_expanded(results_d2q, '/content/output/expanded_d2q.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download expanded files to local machine\n",
    "from google.colab import files\n",
    "\n",
    "files.download('/content/output/expanded_hqfde.tsv')\n",
    "files.download('/content/output/expanded_d2q.tsv')\n",
    "\n",
    "print(\"Download complete! Use these files locally for indexing and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to Google Drive for persistence\n",
    "!cp /content/output/expanded_hqfde.tsv /content/drive/MyDrive/\n",
    "!cp /content/output/expanded_d2q.tsv /content/drive/MyDrive/\n",
    "print(\"Saved to Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}