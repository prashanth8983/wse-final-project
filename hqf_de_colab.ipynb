{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HQF-DE: Document Expansion Pipeline\n",
    "\n",
    "This notebook expands MS MARCO documents using the HQF-DE pipeline:\n",
    "\n",
    "1. **LLM** (Llama-3-8B) → Identifies semantic gaps and generates expansions\n",
    "2. **NLI** (DeBERTa) → Validates expansions are factually consistent\n",
    "3. **Doc2Query** (T5) → Generates synthetic queries\n",
    "4. **Combiner** → Deduplicates and selects best expansions\n",
    "\n",
    "**Input:** `collection_subset.tsv` (1M docs, 331MB)  \n",
    "**Output:** `expanded_hqfde.tsv`, `expanded_d2q.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers sentence-transformers accelerate bitsandbytes pydantic-settings sentencepiece protobuf tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace (required for Llama-3)\n# Add your token to Colab secrets: Key = HF_TOKEN, Value = your_token\nfrom google.colab import userdata\nfrom huggingface_hub import login\n\ntry:\n    token = userdata.get('HF_TOKEN')\n    login(token=token)\n    print(\"Logged in with Colab secret!\")\nexcept:\n    print(\"HF_TOKEN not found in Colab secrets. Add it via the key icon in the left sidebar.\")\n    login()  # Fallback to manual prompt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs('/content/output', exist_ok=True)\n",
    "\n",
    "# Set paths - UPDATE THESE to match your Drive folder\n",
    "DRIVE_FOLDER = \"/content/drive/MyDrive/hqf_de\"  # Change if needed\n",
    "INPUT_FILE = f\"{DRIVE_FOLDER}/collection_subset.tsv\"\n",
    "OUTPUT_HQFDE = f\"{DRIVE_FOLDER}/expanded_hqfde.tsv\"\n",
    "OUTPUT_D2Q = f\"{DRIVE_FOLDER}/expanded_d2q.tsv\"\n",
    "\n",
    "# Verify input file exists\n",
    "if os.path.exists(INPUT_FILE):\n",
    "    print(f\"Found: {INPUT_FILE}\")\n",
    "    !wc -l {INPUT_FILE}\n",
    "else:\n",
    "    print(f\"ERROR: File not found: {INPUT_FILE}\")\n",
    "    print(f\"Please upload collection_subset.tsv to Google Drive at: {DRIVE_FOLDER}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "CACHE = \"/content/cache\"\n",
    "\n",
    "# Load Llama-3-8B (4-bit quantized)\n",
    "print(\"Loading Llama-3-8B...\")\n",
    "llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_name, cache_dir=CACHE)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name, cache_dir=CACHE,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16),\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "llm_pipe = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer, max_new_tokens=256)\n",
    "\n",
    "# Load NLI (DeBERTa)\n",
    "print(\"Loading DeBERTa NLI...\")\n",
    "nli_pipe = pipeline(\"text-classification\", model=\"microsoft/deberta-v3-large-mnli\", device=0, top_k=None)\n",
    "\n",
    "# Load Doc2Query (T5)\n",
    "print(\"Loading Doc2Query...\")\n",
    "d2q_tokenizer = T5Tokenizer.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE)\n",
    "d2q_model = T5ForConditionalGeneration.from_pretrained(\"castorini/doc2query-t5-base-msmarco\", cache_dir=CACHE).to(DEVICE)\n",
    "\n",
    "# Load Sentence-BERT\n",
    "print(\"Loading Sentence-BERT...\")\n",
    "sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "\n",
    "print(\"All models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Expansion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_expand(doc):\n",
    "    \"\"\"Use LLM to identify gaps and generate expansions.\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Analyze this document and generate 3-5 brief factual expansions that add missing context:\n",
    "{doc}\n",
    "\n",
    "Expansions:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    result = llm_pipe(prompt, return_full_text=False, pad_token_id=llm_tokenizer.pad_token_id)\n",
    "    text = result[0][\"generated_text\"]\n",
    "    \n",
    "    # Parse numbered/bulleted list\n",
    "    expansions = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 10:\n",
    "            if line[0].isdigit():\n",
    "                line = line.split(\".\", 1)[-1].strip()\n",
    "            if line.startswith(\"-\"):\n",
    "                line = line[1:].strip()\n",
    "            if line:\n",
    "                expansions.append(line)\n",
    "    return expansions[:5]\n",
    "\n",
    "\n",
    "def nli_validate(doc, expansions, threshold=0.7):\n",
    "    \"\"\"Filter expansions using NLI entailment.\"\"\"\n",
    "    valid = []\n",
    "    for exp in expansions:\n",
    "        try:\n",
    "            result = nli_pipe(f\"{doc} [SEP] {exp}\", truncation=True, max_length=512)\n",
    "            scores = {r[\"label\"].lower(): r[\"score\"] for r in result}\n",
    "            if scores.get(\"entailment\", 0) >= threshold:\n",
    "                valid.append(exp)\n",
    "        except:\n",
    "            pass\n",
    "    return valid\n",
    "\n",
    "\n",
    "def doc2query(doc, n=5):\n",
    "    \"\"\"Generate synthetic queries.\"\"\"\n",
    "    inputs = d2q_tokenizer(doc, max_length=512, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = d2q_model.generate(**inputs, max_length=64, do_sample=True, top_k=10, num_return_sequences=n)\n",
    "    queries = []\n",
    "    for out in outputs:\n",
    "        q = d2q_tokenizer.decode(out, skip_special_tokens=True).strip()\n",
    "        if q and q not in queries:\n",
    "            queries.append(q)\n",
    "    return queries\n",
    "\n",
    "\n",
    "def deduplicate(doc, expansions, threshold=0.85):\n",
    "    \"\"\"Remove duplicate and redundant expansions.\"\"\"\n",
    "    if not expansions:\n",
    "        return []\n",
    "    \n",
    "    embs = sbert.encode(expansions, normalize_embeddings=True)\n",
    "    sim = cosine_similarity(embs)\n",
    "    kept = []\n",
    "    removed = set()\n",
    "    for i in range(len(expansions)):\n",
    "        if i not in removed:\n",
    "            kept.append(expansions[i])\n",
    "            for j in range(i+1, len(expansions)):\n",
    "                if sim[i,j] >= threshold:\n",
    "                    removed.add(j)\n",
    "    \n",
    "    if kept:\n",
    "        doc_emb = sbert.encode([doc], normalize_embeddings=True)\n",
    "        exp_embs = sbert.encode(kept, normalize_embeddings=True)\n",
    "        sims = cosine_similarity(exp_embs, doc_emb).flatten()\n",
    "        kept = [e for e, s in zip(kept, sims) if s < threshold]\n",
    "    \n",
    "    return kept[:10]\n",
    "\n",
    "\n",
    "def expand_hqfde(doc_id, doc):\n",
    "    \"\"\"Full HQF-DE pipeline.\"\"\"\n",
    "    expansions = llm_expand(doc)\n",
    "    valid = nli_validate(doc, expansions)\n",
    "    queries = doc2query(doc)\n",
    "    final = deduplicate(doc, valid + queries)\n",
    "    return doc_id, f\"{doc} {' '.join(final)}\"\n",
    "\n",
    "\n",
    "def expand_d2q_only(doc_id, doc):\n",
    "    \"\"\"Doc2Query baseline only.\"\"\"\n",
    "    queries = doc2query(doc)\n",
    "    return doc_id, f\"{doc} {' '.join(queries)}\"\n",
    "\n",
    "\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test on Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\"\n",
    "\n",
    "print(\"Original:\", test_doc)\n",
    "print()\n",
    "\n",
    "expansions = llm_expand(test_doc)\n",
    "print(\"LLM Expansions:\")\n",
    "for e in expansions:\n",
    "    print(f\"  - {e}\")\n",
    "\n",
    "valid = nli_validate(test_doc, expansions)\n",
    "print(f\"\\nNLI Valid ({len(valid)}/{len(expansions)}):\")\n",
    "for e in valid:\n",
    "    print(f\"  + {e}\")\n",
    "\n",
    "queries = doc2query(test_doc)\n",
    "print(\"\\nDoc2Query:\")\n",
    "for q in queries:\n",
    "    print(f\"  ? {q}\")\n",
    "\n",
    "_, expanded = expand_hqfde(\"test\", test_doc)\n",
    "print(\"\\nFinal Expanded:\")\n",
    "print(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "LIMIT = 100000  # Process 100K docs (set to None for all 1M)\n",
    "\n",
    "# Load documents\n",
    "docs = []\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    for row in csv.reader(f, delimiter='\\t'):\n",
    "        if len(row) >= 2:\n",
    "            docs.append((row[0], row[1]))\n",
    "            if LIMIT and len(docs) >= LIMIT:\n",
    "                break\n",
    "\n",
    "print(f\"Loaded {len(docs):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HQF-DE expansion\n",
    "print(f\"Running HQF-DE expansion on {len(docs):,} docs...\")\n",
    "print(f\"Output: {OUTPUT_HQFDE}\")\n",
    "\n",
    "with open(OUTPUT_HQFDE, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for doc_id, text in tqdm(docs):\n",
    "        try:\n",
    "            did, expanded = expand_hqfde(doc_id, text)\n",
    "            writer.writerow([did, expanded])\n",
    "        except Exception as e:\n",
    "            writer.writerow([doc_id, text])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Doc2Query baseline (for comparison)\n",
    "print(f\"Running Doc2Query baseline on {len(docs):,} docs...\")\n",
    "print(f\"Output: {OUTPUT_D2Q}\")\n",
    "\n",
    "with open(OUTPUT_D2Q, 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for doc_id, text in tqdm(docs):\n",
    "        try:\n",
    "            did, expanded = expand_d2q_only(doc_id, text)\n",
    "            writer.writerow([did, expanded])\n",
    "        except Exception as e:\n",
    "            writer.writerow([doc_id, text])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Output files saved to Google Drive:\")\n",
    "print(f\"  HQF-DE: {OUTPUT_HQFDE} ({os.path.getsize(OUTPUT_HQFDE)/1e6:.1f} MB)\")\n",
    "print(f\"  D2Q:    {OUTPUT_D2Q} ({os.path.getsize(OUTPUT_D2Q)/1e6:.1f} MB)\")\n",
    "print()\n",
    "print(\"Files are automatically synced to your Google Drive.\")\n",
    "print(\"Next: Use these with your C++ indexer for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}