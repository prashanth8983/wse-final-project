{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HQF-DE: Document Expansion Pipeline\n\nProcess the MS MARCO passages subset (1M documents with full qrels coverage) on Colab with GPU.\n\n**Dataset:** `collection_subset.tsv` - 1M documents curated to include all qrels-relevant passages\n\n**Estimated time for 100K docs:** ~5-6 hours on A100, ~15-20 hours on T4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers sentence-transformers scikit-learn pydantic-settings sentencepiece protobuf accelerate bitsandbytes tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to HuggingFace (required for Llama-3 access)\nfrom huggingface_hub import login\nlogin()  # Enter your HuggingFace token when prompted"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoints (important for long runs!)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p /content/drive/MyDrive/hqf_de/output\n",
    "!mkdir -p /content/drive/MyDrive/hqf_de/checkpoints\n",
    "!mkdir -p /content/data /content/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload collection_subset.tsv (1M MS MARCO passages with qrels coverage)\nfrom google.colab import files\nprint(\"Upload your collection_subset.tsv file (1M MS MARCO passages)\")\nprint(\"This file contains all documents referenced in qrels for proper evaluation\")\nuploaded = files.upload()\n\nimport shutil\nfor filename in uploaded.keys():\n    shutil.move(filename, f'/content/data/{filename}')\n    print(f\"Moved {filename} to /content/data/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hqf_de package\n",
    "!mkdir -p hqf_de/models hqf_de/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/config.py\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field\n",
    "from pathlib import Path\n",
    "\n",
    "class HQFDEConfig(BaseSettings):\n",
    "    project_root: Path = Field(default=Path(\"/content\"))\n",
    "    data_dir: Path = Field(default=Path(\"/content/data\"))\n",
    "    output_dir: Path = Field(default=Path(\"/content/drive/MyDrive/hqf_de/output\"))\n",
    "    cache_dir: Path = Field(default=Path(\"/content/cache\"))\n",
    "    checkpoint_dir: Path = Field(default=Path(\"/content/drive/MyDrive/hqf_de/checkpoints\"))\n",
    "\n",
    "    llm_model_name: str = Field(default=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    llm_max_new_tokens: int = Field(default=256)\n",
    "    llm_temperature: float = Field(default=0.7)\n",
    "\n",
    "    doc2query_model: str = Field(default=\"castorini/doc2query-t5-base-msmarco\")\n",
    "    nli_model: str = Field(default=\"microsoft/deberta-v3-large-mnli\")\n",
    "    embedding_model: str = Field(default=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    nli_entailment_threshold: float = Field(default=0.9)\n",
    "    dedup_similarity_threshold: float = Field(default=0.85)\n",
    "    max_doc_length: int = Field(default=512)\n",
    "    device: str = Field(default=\"cuda\")\n",
    "\n",
    "config = HQFDEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/__init__.py\n",
    "from .config import config\n",
    "__all__ = [\"config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/__init__.py\n",
    "from .llm import LLM\n",
    "from .doc2query import Doc2Query\n",
    "from .nli import NLI\n",
    "from .embeddings import Embedder\n",
    "__all__ = [\"LLM\", \"Doc2Query\", \"NLI\", \"Embedder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/llm.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Expansion:\n",
    "    text: str\n",
    "    gaps: List[str]\n",
    "    expansions: List[str]\n",
    "\n",
    "class LLM:\n",
    "    GAP_PROMPT = \"\"\"Analyze this document and list semantic gaps (max 5):\n",
    "{document}\n",
    "\n",
    "Gaps:\"\"\"\n",
    "\n",
    "    EXPAND_PROMPT = \"\"\"Generate brief factual expansions for this document:\n",
    "{document}\n",
    "\n",
    "Gaps: {gaps}\n",
    "\n",
    "Expansions:\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = None, device: str = None, quantize: bool = True):\n",
    "        self.model_name = model or config.llm_model_name\n",
    "        self.device = device or config.device\n",
    "        self.quantize = quantize\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipe = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading LLM: {self.model_name}\")\n",
    "        quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16) if self.quantize else None\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir=config.cache_dir)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, cache_dir=config.cache_dir, quantization_config=quant_config, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer, max_new_tokens=config.llm_max_new_tokens, temperature=config.llm_temperature, do_sample=True)\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def _fmt(self, doc: str, template: str, **kw) -> str:\n",
    "        content = template.format(document=doc, **kw)\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    def _gen(self, prompt: str) -> str:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        result = self.pipe(prompt, return_full_text=False, pad_token_id=self.tokenizer.pad_token_id)\n",
    "        return result[0][\"generated_text\"].strip()\n",
    "\n",
    "    def _parse(self, text: str) -> List[str]:\n",
    "        items = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\"#\"):\n",
    "                if line[0].isdigit():\n",
    "                    line = line.split(\".\", 1)[-1].strip()\n",
    "                if line.startswith(\"-\"):\n",
    "                    line = line[1:].strip()\n",
    "                if line and len(line) > 5:\n",
    "                    items.append(line)\n",
    "        return items[:5]\n",
    "\n",
    "    def gaps(self, doc: str) -> List[str]:\n",
    "        return self._parse(self._gen(self._fmt(doc, self.GAP_PROMPT)))\n",
    "\n",
    "    def expand(self, doc: str, gaps: List[str] = None) -> List[str]:\n",
    "        gaps_text = \"\\n\".join(gaps) if gaps else \"none\"\n",
    "        return self._parse(self._gen(self._fmt(doc, self.EXPAND_PROMPT, gaps=gaps_text)))\n",
    "\n",
    "    def run(self, doc: str) -> Expansion:\n",
    "        g = self.gaps(doc)\n",
    "        e = self.expand(doc, g)\n",
    "        return Expansion(text=doc, gaps=g, expansions=e)\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model, self.tokenizer, self.pipe\n",
    "            self.model = self.tokenizer = self.pipe = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/doc2query.py\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from typing import List\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Doc2Query:\n",
    "    def __init__(self, model: str = None, device: str = None, n: int = 5):\n",
    "        self.model_name = model or config.doc2query_model\n",
    "        self.device = device or config.device\n",
    "        self.n = n\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading doc2query: {self.model_name}\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, cache_dir=config.cache_dir)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name, cache_dir=config.cache_dir).to(self.device)\n",
    "        self.model.eval()\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def gen(self, doc: str, n: int = None) -> List[str]:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        n = n or self.n\n",
    "        inputs = self.tokenizer(doc, max_length=config.max_doc_length, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_length=64, do_sample=True, top_k=10, num_return_sequences=n)\n",
    "        queries = []\n",
    "        for out in outputs:\n",
    "            q = self.tokenizer.decode(out, skip_special_tokens=True).strip()\n",
    "            if q and q not in queries:\n",
    "                queries.append(q)\n",
    "        return queries\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model, self.tokenizer\n",
    "            self.model = self.tokenizer = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/nli.py\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class NLIResult:\n",
    "    hypothesis: str\n",
    "    entailment: float\n",
    "    valid: bool\n",
    "\n",
    "class NLI:\n",
    "    def __init__(self, model: str = None, device: str = None, threshold: float = 0.9):\n",
    "        self.model_name = model or config.nli_model\n",
    "        self.device = device or config.device\n",
    "        self.threshold = threshold\n",
    "        self.pipe = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading NLI: {self.model_name}\")\n",
    "        self.pipe = pipeline(\"text-classification\", model=self.model_name, device=0, top_k=None)\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def check(self, premise: str, hypothesis: str) -> NLIResult:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        try:\n",
    "            results = self.pipe(f\"{premise} [SEP] {hypothesis}\", truncation=True, max_length=512)\n",
    "            scores = {r[\"label\"].lower(): r[\"score\"] for r in results}\n",
    "            ent = scores.get(\"entailment\", 0.0)\n",
    "            return NLIResult(hypothesis=hypothesis, entailment=ent, valid=ent >= self.threshold)\n",
    "        except:\n",
    "            return NLIResult(hypothesis=hypothesis, entailment=0.0, valid=False)\n",
    "\n",
    "    def validate(self, doc: str, expansions: List[str]) -> Tuple[List[str], List[NLIResult]]:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        valid, results = [], []\n",
    "        for exp in expansions:\n",
    "            r = self.check(doc, exp)\n",
    "            results.append(r)\n",
    "            if r.valid:\n",
    "                valid.append(exp)\n",
    "        return valid, results\n",
    "\n",
    "    def unload(self):\n",
    "        if self.pipe:\n",
    "            del self.pipe\n",
    "            self.pipe = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/models/embeddings.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model: str = None, device: str = None):\n",
    "        self.model_name = model or config.embedding_model\n",
    "        self.device = device or config.device\n",
    "        self.model = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        logger.info(f\"Loading embedder: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name, cache_folder=str(config.cache_dir), device=self.device)\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        return self.model.encode(texts, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "    def dedup(self, texts: List[str], threshold: float = 0.85) -> Tuple[List[str], List[int]]:\n",
    "        if len(texts) <= 1:\n",
    "            return texts, list(range(len(texts)))\n",
    "        embs = self.encode(texts)\n",
    "        sim = cosine_similarity(embs)\n",
    "        kept, indices, removed = [], [], set()\n",
    "        for i in range(len(texts)):\n",
    "            if i in removed:\n",
    "                continue\n",
    "            kept.append(texts[i])\n",
    "            indices.append(i)\n",
    "            for j in range(i + 1, len(texts)):\n",
    "                if sim[i, j] >= threshold:\n",
    "                    removed.add(j)\n",
    "        return kept, indices\n",
    "\n",
    "    def dedup_vs_doc(self, doc: str, expansions: List[str], threshold: float = 0.85) -> List[str]:\n",
    "        if not expansions:\n",
    "            return []\n",
    "        doc_emb = self.encode([doc])\n",
    "        exp_embs = self.encode(expansions)\n",
    "        sims = cosine_similarity(exp_embs, doc_emb).flatten()\n",
    "        return [e for e, s in zip(expansions, sims) if s < threshold]\n",
    "\n",
    "    def select(self, expansions: List[str], n: int = 5, doc: str = None) -> List[str]:\n",
    "        if len(expansions) <= n:\n",
    "            return expansions\n",
    "        embs = self.encode(expansions)\n",
    "        if doc:\n",
    "            doc_emb = self.encode([doc])\n",
    "            rel = cosine_similarity(embs, doc_emb).flatten()\n",
    "        else:\n",
    "            rel = np.ones(len(expansions))\n",
    "        selected, indices = [], []\n",
    "        for _ in range(n):\n",
    "            best_idx, best_score = -1, -float('inf')\n",
    "            for i in range(len(expansions)):\n",
    "                if i in indices:\n",
    "                    continue\n",
    "                div = 1 - max(cosine_similarity(embs[i:i+1], embs[indices]).flatten()) if indices else 1.0\n",
    "                score = 0.5 * rel[i] + 0.5 * div\n",
    "                if score > best_score:\n",
    "                    best_score, best_idx = score, i\n",
    "            if best_idx >= 0:\n",
    "                selected.append(expansions[best_idx])\n",
    "                indices.append(best_idx)\n",
    "        return selected\n",
    "\n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            self._loaded = False\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/pipeline/__init__.py\n",
    "from .expander import Expander\n",
    "from .combiner import Combiner\n",
    "__all__ = [\"Expander\", \"Combiner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/pipeline/combiner.py\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "from ..models.embeddings import Embedder\n",
    "\n",
    "GENERIC = {\"information\", \"details\", \"things\", \"stuff\", \"content\", \"topic\", \"subject\", \"matter\", \"example\", \"case\", \"way\", \"method\", \"people\", \"time\", \"place\", \"thing\"}\n",
    "\n",
    "@dataclass\n",
    "class Combined:\n",
    "    original: str\n",
    "    semantic: List[str] = field(default_factory=list)\n",
    "    queries: List[str] = field(default_factory=list)\n",
    "    final: List[str] = field(default_factory=list)\n",
    "    text: str = \"\"\n",
    "\n",
    "class Combiner:\n",
    "    def __init__(self, embedder: Embedder = None, threshold: float = 0.85, max_exp: int = 10):\n",
    "        self.embedder = embedder or Embedder()\n",
    "        self.threshold = threshold\n",
    "        self.max_exp = max_exp\n",
    "        self._loaded = False\n",
    "\n",
    "    def _load(self):\n",
    "        if not self._loaded:\n",
    "            self.embedder.load()\n",
    "            self._loaded = True\n",
    "\n",
    "    def _filter(self, expansions: List[str], doc: str = None) -> List[str]:\n",
    "        out = []\n",
    "        for e in expansions:\n",
    "            e = e.strip()\n",
    "            if not e or len(e.split()) < 3 or len(e.split()) > 50:\n",
    "                continue\n",
    "            words = e.lower().split()\n",
    "            if sum(1 for w in words if w in GENERIC) / len(words) > 0.5:\n",
    "                continue\n",
    "            if doc and e.lower() in doc.lower():\n",
    "                continue\n",
    "            out.append(e)\n",
    "        return out\n",
    "\n",
    "    def combine(self, doc: str, semantic: List[str], queries: List[str]) -> Combined:\n",
    "        self._load()\n",
    "        sem = self._filter(semantic, doc)\n",
    "        q = self._filter(queries, doc)\n",
    "        all_exp = sem + q\n",
    "        if len(all_exp) > 1:\n",
    "            all_exp, _ = self.embedder.dedup(all_exp, self.threshold)\n",
    "            all_exp = self.embedder.dedup_vs_doc(doc, all_exp, self.threshold)\n",
    "        final = self.embedder.select(all_exp, self.max_exp, doc) if len(all_exp) > self.max_exp else all_exp[:self.max_exp]\n",
    "        text = f\"{doc} {' '.join(final)}\"\n",
    "        return Combined(original=doc, semantic=sem, queries=q, final=final, text=text)\n",
    "\n",
    "    def unload(self):\n",
    "        if self._loaded:\n",
    "            self.embedder.unload()\n",
    "            self._loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hqf_de/pipeline/expander.py\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "import logging\n",
    "from ..models.llm import LLM\n",
    "from ..models.nli import NLI\n",
    "from ..models.doc2query import Doc2Query\n",
    "from ..models.embeddings import Embedder\n",
    "from .combiner import Combiner\n",
    "from ..config import config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    doc_id: str\n",
    "    original: str\n",
    "    expanded: str\n",
    "    gaps: List[str] = field(default_factory=list)\n",
    "    raw: List[str] = field(default_factory=list)\n",
    "    valid: List[str] = field(default_factory=list)\n",
    "    queries: List[str] = field(default_factory=list)\n",
    "    final: List[str] = field(default_factory=list)\n",
    "\n",
    "class Expander:\n",
    "    def __init__(self, use_llm: bool = True, use_nli: bool = True, use_d2q: bool = True):\n",
    "        self.llm = LLM() if use_llm else None\n",
    "        self.nli = NLI() if use_nli else None\n",
    "        self.d2q = Doc2Query() if use_d2q else None\n",
    "        self.combiner = Combiner(Embedder())\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        if self._loaded:\n",
    "            return self\n",
    "        if self.llm:\n",
    "            self.llm.load()\n",
    "        if self.nli:\n",
    "            self.nli.load()\n",
    "        if self.d2q:\n",
    "            self.d2q.load()\n",
    "        self._loaded = True\n",
    "        return self\n",
    "\n",
    "    def expand(self, doc_id: str, doc: str) -> Result:\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        result = Result(doc_id=doc_id, original=doc, expanded=doc)\n",
    "        raw = []\n",
    "        if self.llm:\n",
    "            try:\n",
    "                exp = self.llm.run(doc)\n",
    "                result.gaps = exp.gaps\n",
    "                raw = exp.expansions\n",
    "                result.raw = raw\n",
    "            except Exception as e:\n",
    "                logger.error(f\"LLM error: {e}\")\n",
    "        valid = raw\n",
    "        if self.nli and raw:\n",
    "            try:\n",
    "                valid, _ = self.nli.validate(doc, raw)\n",
    "                result.valid = valid\n",
    "            except Exception as e:\n",
    "                logger.error(f\"NLI error: {e}\")\n",
    "        queries = []\n",
    "        if self.d2q:\n",
    "            try:\n",
    "                queries = self.d2q.gen(doc)\n",
    "                result.queries = queries\n",
    "            except Exception as e:\n",
    "                logger.error(f\"D2Q error: {e}\")\n",
    "        try:\n",
    "            combined = self.combiner.combine(doc, valid, queries)\n",
    "            result.final = combined.final\n",
    "            result.expanded = combined.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Combiner error: {e}\")\n",
    "            result.final = (valid + queries)[:10]\n",
    "            result.expanded = f\"{doc} {' '.join(result.final)}\"\n",
    "        return result\n",
    "\n",
    "    def d2q_only(self, doc_id: str, doc: str) -> Result:\n",
    "        if self.d2q and not self.d2q._loaded:\n",
    "            self.d2q.load()\n",
    "        queries = self.d2q.gen(doc) if self.d2q else []\n",
    "        return Result(doc_id=doc_id, original=doc, expanded=f\"{doc} {' '.join(queries)}\", queries=queries, final=queries)\n",
    "\n",
    "    def unload(self):\n",
    "        if self.llm:\n",
    "            self.llm.unload()\n",
    "        if self.nli:\n",
    "            self.nli.unload()\n",
    "        if self.d2q:\n",
    "            self.d2q.unload()\n",
    "        self.combiner.unload()\n",
    "        self._loaded = False\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.load()\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "from hqf_de.pipeline.expander import Expander\n",
    "\n",
    "# Quick test\n",
    "exp = Expander()\n",
    "exp.load()\n",
    "\n",
    "test_doc = \"The Eiffel Tower is a famous landmark in Paris, France. It was built in 1889.\"\n",
    "result = exp.expand(\"test\", test_doc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULT:\")\n",
    "print(f\"Gaps: {result.gaps}\")\n",
    "print(f\"Valid expansions: {result.valid}\")\n",
    "print(f\"Queries: {result.queries}\")\n",
    "print(f\"Final: {result.final}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 100K Documents with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import csv\nimport json\nimport os\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom hqf_de.config import config\n\n# Configuration - Process 100K documents from the 1M subset\nTOTAL_DOCS = 100000  # Change to 1000000 to process all 1M docs\nCHECKPOINT_EVERY = 1000  # Save every 1000 docs\nINPUT_FILE = '/content/data/collection_subset.tsv'  # 1M docs with qrels coverage\nOUTPUT_FILE = str(config.output_dir / 'expanded_hqfde.tsv')\nD2Q_OUTPUT_FILE = str(config.output_dir / 'expanded_d2q.tsv')\nCHECKPOINT_FILE = str(config.checkpoint_dir / 'checkpoint.json')\n\ndef load_docs(path, limit=None, skip=0):\n    \"\"\"Load documents, optionally skipping already processed ones.\"\"\"\n    docs = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for i, row in enumerate(csv.reader(f, delimiter='\\t')):\n            if i < skip:\n                continue\n            if len(row) >= 2:\n                docs.append((row[0], row[1]))\n                if limit and len(docs) >= limit:\n                    break\n    return docs\n\ndef save_checkpoint(processed, output_file):\n    \"\"\"Save checkpoint to resume later.\"\"\"\n    with open(CHECKPOINT_FILE, 'w') as f:\n        json.dump({\n            'processed': processed,\n            'output_file': output_file,\n            'timestamp': datetime.now().isoformat()\n        }, f)\n    print(f\"Checkpoint saved: {processed} documents processed\")\n\ndef load_checkpoint():\n    \"\"\"Load checkpoint if exists.\"\"\"\n    if os.path.exists(CHECKPOINT_FILE):\n        with open(CHECKPOINT_FILE, 'r') as f:\n            return json.load(f)\n    return None\n\ndef append_results(results, path):\n    \"\"\"Append results to TSV file.\"\"\"\n    mode = 'a' if os.path.exists(path) else 'w'\n    with open(path, mode, encoding='utf-8', newline='') as f:\n        w = csv.writer(f, delimiter='\\t')\n        for r in results:\n            w.writerow([r.doc_id, r.expanded])\n\nprint(f\"Configuration:\")\nprint(f\"  Input: {INPUT_FILE}\")\nprint(f\"  Total documents to process: {TOTAL_DOCS:,}\")\nprint(f\"  Checkpoint every: {CHECKPOINT_EVERY:,}\")\nprint(f\"  Output: {OUTPUT_FILE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint\n",
    "checkpoint = load_checkpoint()\n",
    "start_from = 0\n",
    "\n",
    "if checkpoint:\n",
    "    start_from = checkpoint['processed']\n",
    "    print(f\"Resuming from checkpoint: {start_from:,} documents already processed\")\n",
    "    print(f\"Last saved: {checkpoint['timestamp']}\")\n",
    "else:\n",
    "    print(\"Starting fresh - no checkpoint found\")\n",
    "    # Clear output files if starting fresh\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "\n",
    "remaining = TOTAL_DOCS - start_from\n",
    "print(f\"Documents remaining: {remaining:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HQF-DE expansion on 100K documents\n",
    "from hqf_de.pipeline.expander import Expander\n",
    "import time\n",
    "\n",
    "# Load documents\n",
    "print(f\"Loading documents from {start_from:,}...\")\n",
    "docs = load_docs(INPUT_FILE, limit=TOTAL_DOCS-start_from, skip=start_from)\n",
    "print(f\"Loaded {len(docs):,} documents\")\n",
    "\n",
    "# Initialize expander (models already loaded from test)\n",
    "if not exp._loaded:\n",
    "    exp = Expander()\n",
    "    exp.load()\n",
    "\n",
    "# Process with checkpointing\n",
    "batch_results = []\n",
    "processed = start_from\n",
    "start_time = time.time()\n",
    "\n",
    "for i, (doc_id, text) in enumerate(tqdm(docs, desc=\"HQF-DE Expansion\")):\n",
    "    try:\n",
    "        result = exp.expand(doc_id, text)\n",
    "        batch_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on doc {doc_id}: {e}\")\n",
    "        # Save original doc on error\n",
    "        from hqf_de.pipeline.expander import Result\n",
    "        batch_results.append(Result(doc_id=doc_id, original=text, expanded=text))\n",
    "    \n",
    "    # Checkpoint every N documents\n",
    "    if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "        append_results(batch_results, OUTPUT_FILE)\n",
    "        processed += len(batch_results)\n",
    "        save_checkpoint(processed, OUTPUT_FILE)\n",
    "        \n",
    "        # Print stats\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed\n",
    "        remaining_docs = len(docs) - (i + 1)\n",
    "        eta = remaining_docs / rate if rate > 0 else 0\n",
    "        print(f\"  Rate: {rate:.2f} docs/sec | ETA: {eta/3600:.1f} hours\")\n",
    "        \n",
    "        batch_results = []\n",
    "\n",
    "# Save final batch\n",
    "if batch_results:\n",
    "    append_results(batch_results, OUTPUT_FILE)\n",
    "    processed += len(batch_results)\n",
    "    save_checkpoint(processed, OUTPUT_FILE)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nHQF-DE Complete!\")\n",
    "print(f\"Total processed: {processed:,} documents\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run Doc2Query-only baseline for comparison\n",
    "print(\"\\nRunning Doc2Query-only baseline...\")\n",
    "\n",
    "# Unload full pipeline, load d2q-only\n",
    "exp.unload()\n",
    "exp_d2q = Expander(use_llm=False, use_nli=False, use_d2q=True)\n",
    "exp_d2q.load()\n",
    "\n",
    "# Reload all docs for d2q\n",
    "docs = load_docs(INPUT_FILE, limit=TOTAL_DOCS)\n",
    "print(f\"Processing {len(docs):,} documents with Doc2Query only...\")\n",
    "\n",
    "d2q_results = []\n",
    "for doc_id, text in tqdm(docs, desc=\"Doc2Query\"):\n",
    "    result = exp_d2q.d2q_only(doc_id, text)\n",
    "    d2q_results.append(result)\n",
    "\n",
    "# Save d2q results\n",
    "with open(D2Q_OUTPUT_FILE, 'w', encoding='utf-8', newline='') as f:\n",
    "    w = csv.writer(f, delimiter='\\t')\n",
    "    for r in d2q_results:\n",
    "        w.writerow([r.doc_id, r.expanded])\n",
    "\n",
    "print(f\"Doc2Query baseline saved: {D2Q_OUTPUT_FILE}\")\n",
    "exp_d2q.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download results\nfrom google.colab import files\n\nprint(\"Downloading expanded files...\")\nfiles.download(OUTPUT_FILE)\nfiles.download(D2Q_OUTPUT_FILE)\n\nprint(\"\\nDone! Files downloaded:\")\nprint(f\"  1. expanded_hqfde.tsv - Full HQF-DE pipeline\")\nprint(f\"  2. expanded_d2q.tsv - Doc2Query baseline\")\nprint(f\"\\nThese files contain docs from collection_subset.tsv which has 100% qrels coverage.\")\nprint(f\"Use these with your C++ indexer for evaluation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print summary statistics\nimport os\n\nhqf_size = os.path.getsize(OUTPUT_FILE) / (1024*1024)\nd2q_size = os.path.getsize(D2Q_OUTPUT_FILE) / (1024*1024)\n\nprint(\"=\"*60)\nprint(\"SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Documents processed: {TOTAL_DOCS:,}\")\nprint(f\"Input file: collection_subset.tsv (1M docs with 100% qrels coverage)\")\nprint(f\"\")\nprint(f\"HQF-DE output: {hqf_size:.1f} MB\")\nprint(f\"Doc2Query output: {d2q_size:.1f} MB\")\nprint(f\"\")\nprint(\"Next steps:\")\nprint(\"  1. Download the TSV files\")\nprint(\"  2. Run your C++ indexer on both files\")\nprint(\"  3. Evaluate with qrels.dev.tsv, qrels.eval.one.tsv, qrels.eval.two.tsv\")\nprint(\"  4. Compare nDCG@10 and Recall@1000 between HQF-DE and Doc2Query\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}