{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nli = pipeline(\"text-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original and expanded documents\n",
    "originals = {}\n",
    "with open('/content/drive/MyDrive/hqf_de/collection_100k.tsv') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            originals[parts[0]] = parts[1]\n",
    "\n",
    "expanded = []\n",
    "with open('/content/drive/MyDrive/hqf_de/expanded_100k.tsv') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t', 1)\n",
    "        if len(parts) == 2:\n",
    "            expanded.append((parts[0], parts[1]))\n",
    "\n",
    "print(f\"Originals: {len(originals):,}, Expanded: {len(expanded):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_expansions(original, expanded_text):\n",
    "    \"\"\"Extract added sentences from expanded text.\"\"\"\n",
    "    if not expanded_text.startswith(original):\n",
    "        return []\n",
    "    added = expanded_text[len(original):].strip()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', added)\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 15]\n",
    "\n",
    "def is_not_contradiction(premise, hypothesis):\n",
    "    \"\"\"Return True if hypothesis doesn't contradict premise.\"\"\"\n",
    "    result = nli(f\"{premise}</s></s>{hypothesis}\", truncation=True)\n",
    "    return result[0]['label'] != 'contradiction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/content/drive/MyDrive/hqf_de/validated_100k.tsv'\n",
    "\n",
    "kept, removed = 0, 0\n",
    "\n",
    "with open(output_path, 'w') as out:\n",
    "    for doc_id, exp_text in tqdm(expanded):\n",
    "        orig = originals.get(doc_id, \"\")\n",
    "        if not orig:\n",
    "            out.write(f\"{doc_id}\\t{exp_text}\\n\")\n",
    "            continue\n",
    "        \n",
    "        expansions = get_expansions(orig, exp_text)\n",
    "        valid = [e for e in expansions if is_not_contradiction(orig, e)]\n",
    "        \n",
    "        kept += len(valid)\n",
    "        removed += len(expansions) - len(valid)\n",
    "        \n",
    "        final = orig + (\" \" + \" \".join(valid) if valid else \"\")\n",
    "        out.write(f\"{doc_id}\\t{final}\\n\")\n",
    "\n",
    "print(f\"\\nKept: {kept:,}, Removed: {removed:,}\")\n",
    "print(f\"Keep rate: {kept / (kept + removed) * 100:.1f}%\" if kept + removed > 0 else \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
